{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a 5-class classification problem\n",
      "====The shape of data : X : (7799, 32, 32, 3) , y : (7799,)====\n"
     ]
    }
   ],
   "source": [
    "# Import the converted model's class\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from vggnet import VGG_FACE_16_layers as MyNet\n",
    "\n",
    "# load image and labels\n",
    "X = np.load(\"../../outfile_x.npy\")\n",
    "y = np.load(\"../../outfile_y.npy\").astype(int)\n",
    "n_class= len(np.unique(y))\n",
    "print \"It's a {}-class classification problem\".format(str(n_class))\n",
    "print \"====The shape of data : X : {} , y : {}====\".format(str(X.shape),str(y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 275, 1: 6568, 2: 326, 3: 365, 4: 265})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y)\n",
    "# since 1 is to many sample 1000 out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label1=np.where(y==1)\n",
    "np.random.shuffle(label1)\n",
    "others=np.where(y!=1)\n",
    "label_list=list(label1[0][:4000])+list(others[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5231, 32, 32, 3)\n",
      "(5231,)\n"
     ]
    }
   ],
   "source": [
    "# limit number of label1\n",
    "X=X[label_list]\n",
    "y=y[label_list]\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.0001)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec1] fintune from freeze pool5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images_raw = tf.placeholder(tf.float32, shape=[None, 32,32,3])\n",
    "images = tf.image.resize_images(images_raw,(224,224))\n",
    "\n",
    "# concate network\n",
    "net = MyNet({'data': images})\n",
    "pool5 = net.layers['pool5']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Load the data\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    net.load('vggnet.npy', sess)\n",
    "    batch_size=30\n",
    "    idx=0\n",
    "    result_list=[]\n",
    "    while idx*batch_size<len(X):\n",
    "        range_idx=range(idx*batch_size, min( (idx+1)*batch_size,len(X)))\n",
    "        np_images =X[range_idx]\n",
    "        feed = {images_raw: np_images}\n",
    "        pool5_result = sess.run(pool5, feed_dict=feed)\n",
    "        result_list.append(pool5_result)\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pool5=np.concatenate(result_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5231, 7, 7, 512)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pool5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 4184 images in training set\n",
      "there are 1047 images in testing set\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(len(X))\n",
    "np.random.shuffle(arr)\n",
    "X=X_pool5[arr]\n",
    "y=y[arr]\n",
    "data={\n",
    "  'X_train': X[:int(len(X)*0.8)],\n",
    "  'y_train': y[:int(len(X)*0.8)],\n",
    "  'X_val': X[int(len(X)*0.8):],\n",
    "  'y_val': y[int(len(X)*0.8):],\n",
    "}\n",
    "n_val = len(data['X_val'])\n",
    "n_train = len(data['X_train'])\n",
    "\n",
    "print \"there are \"+ str(data['X_train'].shape[0]) + \" images in training set\"\n",
    "print \"there are \"+ str(data['X_val'].shape[0]) + \" images in testing set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hidden_n=4096\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "X_pool5_ph = tf.placeholder(tf.float32, shape=[None,7,7,512],name='feature_x')\n",
    "labels_raw = tf.placeholder(tf.int32, shape=[None,],name='truth_y')\n",
    "labels = tf.one_hot(labels_raw,n_class)\n",
    "\n",
    "with tf.name_scope('fintune_whimh'):\n",
    "    with tf.name_scope('fc6'):\n",
    "        pool5_flat_size = int(np.prod(X_pool5_ph.shape[1:]))\n",
    "        pool5_flat = tf.reshape(X_pool5_ph, [-1, pool5_flat_size])\n",
    "        W_fc6 = weight_variable([pool5_flat_size, Hidden_n])\n",
    "        b_fc6 = bias_variable([Hidden_n])\n",
    "        H_fc6=tf.nn.relu(tf.matmul(pool5_flat, W_fc6) + b_fc6) \n",
    "        H_fc6 = tf.nn.dropout(H_fc6, keep_prob)\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        #calculate_entropy\n",
    "        W_fc7 = weight_variable([Hidden_n, n_class])\n",
    "        b_fc7 = bias_variable([n_class])\n",
    "        fc7 = tf.matmul(H_fc6, W_fc7) + b_fc7 \n",
    "        \n",
    "        predctions = tf.argmax(fc7,1, name='predictions')\n",
    "        ground_truth = tf.argmax(labels,1)\n",
    "    with tf.name_scope('regular'):    \n",
    "        regularizers = (tf.nn.l2_loss(W_fc7) + tf.nn.l2_loss(b_fc7)+\n",
    "                        (tf.nn.l2_loss(W_fc6) + tf.nn.l2_loss(b_fc6)))\n",
    "        \n",
    "correct_prediction = tf.equal(predctions, ground_truth)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=fc7 ,labels= labels), 0)+regularizers*1e-3\n",
    "train_op = tf.train.AdamOptimizer(1e-6).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Iteration: ', 0, 'training-acc', 0.25999999, 'vali-loss', 1.6050611, 'vali-acc', 0.76981854)\n",
      "('Iteration: ', 30, 'training-acc', 0.77999997, 'vali-loss', 1.3698556, 'vali-acc', 0.76981854)\n",
      "('Iteration: ', 60, 'training-acc', 0.75999999, 'vali-loss', 1.0317177, 'vali-acc', 0.76981854)\n",
      "('Iteration: ', 90, 'training-acc', 0.78999996, 'vali-loss', 0.86502689, 'vali-acc', 0.76981848)\n",
      "('Iteration: ', 120, 'training-acc', 0.71000004, 'vali-loss', 0.76324797, 'vali-acc', 0.76981848)\n",
      "('Iteration: ', 150, 'training-acc', 0.76999992, 'vali-loss', 0.68597698, 'vali-acc', 0.77268386)\n",
      "('Iteration: ', 180, 'training-acc', 0.80999994, 'vali-loss', 0.6258117, 'vali-acc', 0.81757402)\n",
      "('Iteration: ', 210, 'training-acc', 0.80000001, 'vali-loss', 0.57759732, 'vali-acc', 0.84431702)\n",
      "('Iteration: ', 240, 'training-acc', 0.89000005, 'vali-loss', 0.54045975, 'vali-acc', 0.85482335)\n",
      "('Iteration: ', 270, 'training-acc', 0.84000003, 'vali-loss', 0.50377017, 'vali-acc', 0.85864365)\n",
      "('Iteration: ', 300, 'training-acc', 0.88, 'vali-loss', 0.47222006, 'vali-acc', 0.86150908)\n",
      "('Iteration: ', 330, 'training-acc', 0.87999994, 'vali-loss', 0.44507626, 'vali-acc', 0.86628461)\n",
      "('Iteration: ', 360, 'training-acc', 0.88999999, 'vali-loss', 0.41918209, 'vali-acc', 0.87106019)\n",
      "('Iteration: ', 390, 'training-acc', 0.83000004, 'vali-loss', 0.39573127, 'vali-acc', 0.88156641)\n",
      "('Iteration: ', 420, 'training-acc', 0.87999994, 'vali-loss', 0.37603289, 'vali-acc', 0.89016235)\n",
      "('Iteration: ', 450, 'training-acc', 0.82999992, 'vali-loss', 0.35523486, 'vali-acc', 0.89111745)\n",
      "('Iteration: ', 480, 'training-acc', 0.92000008, 'vali-loss', 0.33834055, 'vali-acc', 0.89780331)\n",
      "('Iteration: ', 510, 'training-acc', 0.94000006, 'vali-loss', 0.32494476, 'vali-acc', 0.91117477)\n",
      "('Iteration: ', 540, 'training-acc', 0.90999997, 'vali-loss', 0.31026781, 'vali-acc', 0.91212988)\n",
      "('Iteration: ', 570, 'training-acc', 0.93000001, 'vali-loss', 0.29533243, 'vali-acc', 0.91690546)\n",
      "('Iteration: ', 600, 'training-acc', 0.93000001, 'vali-loss', 0.28434244, 'vali-acc', 0.92072588)\n",
      "('Iteration: ', 630, 'training-acc', 0.91000003, 'vali-loss', 0.27502692, 'vali-acc', 0.9245463)\n",
      "('Iteration: ', 660, 'training-acc', 0.94000006, 'vali-loss', 0.26557395, 'vali-acc', 0.92645651)\n",
      "('Iteration: ', 690, 'training-acc', 0.93000001, 'vali-loss', 0.25849468, 'vali-acc', 0.93123209)\n",
      "('Iteration: ', 720, 'training-acc', 0.94999999, 'vali-loss', 0.25005776, 'vali-acc', 0.93123209)\n",
      "('Iteration: ', 750, 'training-acc', 0.99000001, 'vali-loss', 0.24337396, 'vali-acc', 0.9331423)\n",
      "('Iteration: ', 780, 'training-acc', 0.96000004, 'vali-loss', 0.23702925, 'vali-acc', 0.93505251)\n",
      "('Iteration: ', 810, 'training-acc', 0.93000007, 'vali-loss', 0.23124115, 'vali-acc', 0.94173831)\n",
      "('Iteration: ', 840, 'training-acc', 0.94999999, 'vali-loss', 0.22558092, 'vali-acc', 0.93982804)\n",
      "('Iteration: ', 870, 'training-acc', 0.88999999, 'vali-loss', 0.22059675, 'vali-acc', 0.94173825)\n",
      "('Iteration: ', 900, 'training-acc', 0.99000001, 'vali-loss', 0.21539849, 'vali-acc', 0.94269335)\n",
      "('Iteration: ', 930, 'training-acc', 0.97000003, 'vali-loss', 0.21023445, 'vali-acc', 0.94364846)\n",
      "('Iteration: ', 960, 'training-acc', 0.97999996, 'vali-loss', 0.20601112, 'vali-acc', 0.94746888)\n",
      "('Iteration: ', 990, 'training-acc', 0.98999995, 'vali-loss', 0.20266579, 'vali-acc', 0.94746894)\n",
      "('Iteration: ', 1020, 'training-acc', 0.98000002, 'vali-loss', 0.19911124, 'vali-acc', 0.94746894)\n",
      "('Iteration: ', 1050, 'training-acc', 0.95000005, 'vali-loss', 0.19508506, 'vali-acc', 0.95033425)\n",
      "('Iteration: ', 1080, 'training-acc', 0.98000002, 'vali-loss', 0.1918381, 'vali-acc', 0.95128936)\n",
      "('Iteration: ', 1110, 'training-acc', 0.97000003, 'vali-loss', 0.18941669, 'vali-acc', 0.94842404)\n",
      "('Iteration: ', 1140, 'training-acc', 0.94, 'vali-loss', 0.18671605, 'vali-acc', 0.95033419)\n",
      "('Iteration: ', 1170, 'training-acc', 0.97999996, 'vali-loss', 0.18465762, 'vali-acc', 0.9512893)\n",
      "('Iteration: ', 1200, 'training-acc', 0.99000001, 'vali-loss', 0.18229857, 'vali-acc', 0.95128936)\n",
      "('Iteration: ', 1230, 'training-acc', 0.97000003, 'vali-loss', 0.18065585, 'vali-acc', 0.95128942)\n",
      "('Iteration: ', 1260, 'training-acc', 0.97000003, 'vali-loss', 0.1782199, 'vali-acc', 0.95415467)\n",
      "('Iteration: ', 1290, 'training-acc', 0.96999997, 'vali-loss', 0.17552958, 'vali-acc', 0.95319957)\n",
      "('Iteration: ', 1320, 'training-acc', 0.94000006, 'vali-loss', 0.17436136, 'vali-acc', 0.95606494)\n",
      "('Iteration: ', 1350, 'training-acc', 0.97000003, 'vali-loss', 0.17201203, 'vali-acc', 0.95510983)\n",
      "('Iteration: ', 1380, 'training-acc', 0.98000002, 'vali-loss', 0.17022733, 'vali-acc', 0.95702004)\n",
      "('Iteration: ', 1410, 'training-acc', 0.98000002, 'vali-loss', 0.16893555, 'vali-acc', 0.95606488)\n",
      "('Iteration: ', 1440, 'training-acc', 0.98000002, 'vali-loss', 0.16763867, 'vali-acc', 0.95702004)\n",
      "('Iteration: ', 1470, 'training-acc', 0.96000004, 'vali-loss', 0.16526341, 'vali-acc', 0.95797509)\n",
      "('Iteration: ', 1500, 'training-acc', 0.97000003, 'vali-loss', 0.16309209, 'vali-acc', 0.95606482)\n",
      "('Iteration: ', 1530, 'training-acc', 0.96000004, 'vali-loss', 0.16261698, 'vali-acc', 0.95701998)\n",
      "('Iteration: ', 1560, 'training-acc', 0.97999996, 'vali-loss', 0.16166766, 'vali-acc', 0.95510983)\n",
      "('Iteration: ', 1590, 'training-acc', 1.0, 'vali-loss', 0.16002098, 'vali-acc', 0.95797509)\n",
      "('Iteration: ', 1620, 'training-acc', 0.97000003, 'vali-loss', 0.15924005, 'vali-acc', 0.95797515)\n",
      "('Iteration: ', 1650, 'training-acc', 0.94, 'vali-loss', 0.15789071, 'vali-acc', 0.95988536)\n",
      "('Iteration: ', 1680, 'training-acc', 0.97000003, 'vali-loss', 0.15588297, 'vali-acc', 0.9598853)\n",
      "('Iteration: ', 1710, 'training-acc', 0.99000001, 'vali-loss', 0.15504472, 'vali-acc', 0.95988536)\n",
      "('Iteration: ', 1740, 'training-acc', 0.99999994, 'vali-loss', 0.1546106, 'vali-acc', 0.9608404)\n",
      "('Iteration: ', 1770, 'training-acc', 0.97999996, 'vali-loss', 0.1536911, 'vali-acc', 0.9608404)\n",
      "('Iteration: ', 1800, 'training-acc', 0.99000001, 'vali-loss', 0.15256208, 'vali-acc', 0.96084046)\n",
      "('Iteration: ', 1830, 'training-acc', 0.99999994, 'vali-loss', 0.15255979, 'vali-acc', 0.9598853)\n",
      "('Iteration: ', 1860, 'training-acc', 0.97000003, 'vali-loss', 0.15146965, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 1890, 'training-acc', 0.98000002, 'vali-loss', 0.15039317, 'vali-acc', 0.96084046)\n",
      "('Iteration: ', 1920, 'training-acc', 0.97999996, 'vali-loss', 0.14998533, 'vali-acc', 0.9608404)\n",
      "('Iteration: ', 1950, 'training-acc', 0.97999996, 'vali-loss', 0.14877616, 'vali-acc', 0.96084046)\n",
      "('Iteration: ', 1980, 'training-acc', 1.0, 'vali-loss', 0.1476973, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 2010, 'training-acc', 0.99000001, 'vali-loss', 0.14776148, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 2040, 'training-acc', 0.99000001, 'vali-loss', 0.14648652, 'vali-acc', 0.96370578)\n",
      "('Iteration: ', 2070, 'training-acc', 0.98000002, 'vali-loss', 0.14517812, 'vali-acc', 0.96370578)\n",
      "('Iteration: ', 2100, 'training-acc', 0.99000001, 'vali-loss', 0.14502177, 'vali-acc', 0.96370578)\n",
      "('Iteration: ', 2130, 'training-acc', 1.0, 'vali-loss', 0.14485371, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 2160, 'training-acc', 0.98000002, 'vali-loss', 0.14349833, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 2190, 'training-acc', 0.99000001, 'vali-loss', 0.14387146, 'vali-acc', 0.96370578)\n",
      "('Iteration: ', 2220, 'training-acc', 0.98000002, 'vali-loss', 0.14468585, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 2250, 'training-acc', 0.99000001, 'vali-loss', 0.14333098, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 2280, 'training-acc', 1.0, 'vali-loss', 0.14324737, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 2310, 'training-acc', 0.99000001, 'vali-loss', 0.14304973, 'vali-acc', 0.96084046)\n",
      "('Iteration: ', 2340, 'training-acc', 1.0, 'vali-loss', 0.14212362, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 2370, 'training-acc', 0.99000001, 'vali-loss', 0.14243311, 'vali-acc', 0.96084046)\n",
      "('Iteration: ', 2400, 'training-acc', 1.0, 'vali-loss', 0.14306647, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 2430, 'training-acc', 1.0, 'vali-loss', 0.14254642, 'vali-acc', 0.96370578)\n",
      "('Iteration: ', 2460, 'training-acc', 0.97999996, 'vali-loss', 0.14186198, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 2490, 'training-acc', 1.0, 'vali-loss', 0.1402189, 'vali-acc', 0.96370578)\n",
      "('Iteration: ', 2520, 'training-acc', 0.98000002, 'vali-loss', 0.14045902, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 2550, 'training-acc', 0.99000001, 'vali-loss', 0.14003471, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 2580, 'training-acc', 1.0, 'vali-loss', 0.13922557, 'vali-acc', 0.96275061)\n",
      "('Iteration: ', 2610, 'training-acc', 0.98000002, 'vali-loss', 0.1389619, 'vali-acc', 0.96370578)\n",
      "('Iteration: ', 2640, 'training-acc', 1.0, 'vali-loss', 0.13811056, 'vali-acc', 0.96275061)\n",
      "('Iteration: ', 2670, 'training-acc', 0.99000001, 'vali-loss', 0.13845368, 'vali-acc', 0.96466088)\n",
      "('Iteration: ', 2700, 'training-acc', 1.0, 'vali-loss', 0.1384103, 'vali-acc', 0.96466088)\n",
      "('Iteration: ', 2730, 'training-acc', 1.0, 'vali-loss', 0.13896702, 'vali-acc', 0.96561599)\n",
      "('Iteration: ', 2760, 'training-acc', 1.0, 'vali-loss', 0.13976257, 'vali-acc', 0.96466088)\n",
      "('Iteration: ', 2790, 'training-acc', 0.99000001, 'vali-loss', 0.1391128, 'vali-acc', 0.96561599)\n",
      "('Iteration: ', 2820, 'training-acc', 0.99000001, 'vali-loss', 0.13936195, 'vali-acc', 0.96466088)\n",
      "('Iteration: ', 2850, 'training-acc', 0.99000001, 'vali-loss', 0.1380315, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 2880, 'training-acc', 0.99999994, 'vali-loss', 0.13701361, 'vali-acc', 0.96561599)\n",
      "('Iteration: ', 2910, 'training-acc', 1.0, 'vali-loss', 0.13896737, 'vali-acc', 0.96561599)\n",
      "('Iteration: ', 2940, 'training-acc', 0.99000001, 'vali-loss', 0.13849197, 'vali-acc', 0.96370578)\n",
      "('Iteration: ', 2970, 'training-acc', 0.98999995, 'vali-loss', 0.13796943, 'vali-acc', 0.96370578)\n",
      "('Iteration: ', 3000, 'training-acc', 0.99000001, 'vali-loss', 0.13878524, 'vali-acc', 0.96466088)\n",
      "('Iteration: ', 3030, 'training-acc', 0.99999994, 'vali-loss', 0.13749802, 'vali-acc', 0.96370578)\n",
      "('Iteration: ', 3060, 'training-acc', 1.0, 'vali-loss', 0.13893434, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3090, 'training-acc', 0.99000001, 'vali-loss', 0.13792631, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 3120, 'training-acc', 1.0, 'vali-loss', 0.13884708, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3150, 'training-acc', 1.0, 'vali-loss', 0.13830562, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3180, 'training-acc', 0.99000001, 'vali-loss', 0.1390543, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3210, 'training-acc', 1.0, 'vali-loss', 0.13830042, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3240, 'training-acc', 0.99000001, 'vali-loss', 0.136859, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3270, 'training-acc', 1.0, 'vali-loss', 0.13724709, 'vali-acc', 0.96370578)\n",
      "('Iteration: ', 3300, 'training-acc', 1.0, 'vali-loss', 0.13784334, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 3330, 'training-acc', 0.99000001, 'vali-loss', 0.13811612, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 3360, 'training-acc', 0.99000001, 'vali-loss', 0.13844211, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3390, 'training-acc', 1.0, 'vali-loss', 0.13682497, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 3420, 'training-acc', 0.99000001, 'vali-loss', 0.13665354, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 3450, 'training-acc', 1.0, 'vali-loss', 0.13701107, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3480, 'training-acc', 1.0, 'vali-loss', 0.13902819, 'vali-acc', 0.96084046)\n",
      "('Iteration: ', 3510, 'training-acc', 0.99000001, 'vali-loss', 0.13686353, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 3540, 'training-acc', 0.99999994, 'vali-loss', 0.13695762, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 3570, 'training-acc', 0.99000001, 'vali-loss', 0.1386825, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3600, 'training-acc', 1.0, 'vali-loss', 0.13664131, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 3630, 'training-acc', 1.0, 'vali-loss', 0.13676526, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 3660, 'training-acc', 1.0, 'vali-loss', 0.13894068, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3690, 'training-acc', 1.0, 'vali-loss', 0.1394151, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 3720, 'training-acc', 1.0, 'vali-loss', 0.13916054, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3750, 'training-acc', 1.0, 'vali-loss', 0.14080335, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 3780, 'training-acc', 1.0, 'vali-loss', 0.14133461, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3810, 'training-acc', 1.0, 'vali-loss', 0.13961886, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 3840, 'training-acc', 1.0, 'vali-loss', 0.13971291, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3870, 'training-acc', 1.0, 'vali-loss', 0.14240521, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3900, 'training-acc', 0.99000001, 'vali-loss', 0.13924025, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 3930, 'training-acc', 1.0, 'vali-loss', 0.13909438, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3960, 'training-acc', 1.0, 'vali-loss', 0.13962337, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 3990, 'training-acc', 1.0, 'vali-loss', 0.14006683, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 4020, 'training-acc', 1.0, 'vali-loss', 0.141249, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 4050, 'training-acc', 1.0, 'vali-loss', 0.14097013, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 4080, 'training-acc', 1.0, 'vali-loss', 0.13920476, 'vali-acc', 0.96084046)\n",
      "('Iteration: ', 4110, 'training-acc', 1.0, 'vali-loss', 0.13958155, 'vali-acc', 0.96275067)\n",
      "('Iteration: ', 4140, 'training-acc', 0.99000001, 'vali-loss', 0.13977246, 'vali-acc', 0.96370578)\n",
      "('Iteration: ', 4170, 'training-acc', 1.0, 'vali-loss', 0.13863885, 'vali-acc', 0.95988536)\n",
      "('Iteration: ', 4200, 'training-acc', 1.0, 'vali-loss', 0.13963184, 'vali-acc', 0.96179557)\n",
      "('Iteration: ', 4230, 'training-acc', 0.98999995, 'vali-loss', 0.13893135, 'vali-acc', 0.96084046)\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "with tf.Session() as sess:\n",
    "    # Load the data\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(7000):\n",
    "        arr = np.arange(n_train)\n",
    "        np.random.shuffle(arr)\n",
    "        np_images, np_labels = (data['X_train'][arr[:batch_size]] ,data['y_train'][arr[:batch_size]])\n",
    "        feed = {X_pool5_ph: np_images, labels_raw: np_labels, keep_prob:0.4}\n",
    "        np_loss,acc, _ = sess.run([loss,accuracy, train_op], feed_dict=feed)\n",
    "        if i % 30 == 0:\n",
    "            arr = np.arange(n_val)\n",
    "            np.random.shuffle(arr)\n",
    "            np_images, np_labels = (data['X_val'] ,data['y_val'])\n",
    "            feed = {X_pool5_ph: np_images, labels_raw: np_labels, keep_prob:1.0}\n",
    "            vali_loss, vali_acc = sess.run([loss,accuracy], feed_dict=feed)\n",
    "            print('Iteration: ', i,\"training-acc\",acc,\"vali-loss\", vali_loss, \"vali-acc\", vali_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [Sec2] Finetune End2end\n",
    "skip sec1 after reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 4184 images in training set\n",
      "there are 1047 images in testing set\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(len(X))\n",
    "np.random.shuffle(arr)\n",
    "X=X[arr]\n",
    "y=y[arr]\n",
    "data={\n",
    "  'X_train': X[:int(len(X)*0.8)],\n",
    "  'y_train': y[:int(len(X)*0.8)],\n",
    "  'X_val': X[int(len(X)*0.8):],\n",
    "  'y_val': y[int(len(X)*0.8):],\n",
    "}\n",
    "n_val = len(data['X_val'])\n",
    "n_train = len(data['X_train'])\n",
    "print \"there are \"+ str(data['X_train'].shape[0]) + \" images in training set\"\n",
    "print \"there are \"+ str(data['X_val'].shape[0]) + \" images in testing set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hidden_n=1024\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "images_raw = tf.placeholder(tf.float32, shape=[None, 32,32,3])\n",
    "images = tf.image.resize_images(images_raw,(224,224))\n",
    "labels_raw = tf.placeholder(tf.int32, shape=[None,],name='truth_y')\n",
    "labels = tf.one_hot(labels_raw,n_class)\n",
    "\n",
    "# concate network\n",
    "net = MyNet({'data': images})\n",
    "pool5 = net.layers['pool5']\n",
    "with tf.name_scope('fintune_whimh'):\n",
    "    with tf.name_scope('fc6'):\n",
    "        pool5_flat_size = int(np.prod(pool5.shape[1:]))\n",
    "        pool5_flat = tf.reshape(pool5, [-1, pool5_flat_size])\n",
    "        W_fc6 = weight_variable([pool5_flat_size, Hidden_n])\n",
    "        b_fc6 = bias_variable([Hidden_n])\n",
    "        H_fc6=tf.nn.relu(tf.matmul(pool5_flat, W_fc6) + b_fc6) \n",
    "        H_fc6 = tf.nn.dropout(H_fc6, keep_prob)\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        #calculate_entropy\n",
    "        W_fc7 = weight_variable([Hidden_n, n_class])\n",
    "        b_fc7 = bias_variable([n_class])\n",
    "        fc7 = tf.matmul(H_fc6, W_fc7) + b_fc7 \n",
    "        \n",
    "        predctions = tf.argmax(fc7,1, name='predictions')\n",
    "        ground_truth = tf.argmax(labels,1)\n",
    "    with tf.name_scope('regular'):    \n",
    "        regularizers = (tf.nn.l2_loss(W_fc7) + tf.nn.l2_loss(b_fc7)+\n",
    "                        (tf.nn.l2_loss(W_fc6) + tf.nn.l2_loss(b_fc6)))\n",
    "\n",
    "correct_prediction = tf.equal(predctions, ground_truth)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=fc7 ,labels= labels), 0)+regularizers*1e-3\n",
    "train_op = tf.train.AdamOptimizer(1e-6).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Iteration: ', 0, 'training-acc', 0.033333335, 'vali-loss', 1.6086761, 'vali-acc', 0.75481492)\n",
      "('Iteration: ', 300, 'training-acc', 0.86666673, 'vali-loss', 0.52611846, 'vali-acc', 0.83978844)\n",
      "('Iteration: ', 600, 'training-acc', 0.9666667, 'vali-loss', 0.26976711, 'vali-acc', 0.9189418)\n",
      "('Iteration: ', 900, 'training-acc', 1.0, 'vali-loss', 0.20543028, 'vali-acc', 0.94941807)\n",
      "('Iteration: ', 1200, 'training-acc', 0.9666667, 'vali-loss', 0.19890976, 'vali-acc', 0.95132273)\n",
      "('Iteration: ', 1500, 'training-acc', 0.9666667, 'vali-loss', 0.16905002, 'vali-acc', 0.95216936)\n",
      "('Iteration: ', 1800, 'training-acc', 1.0, 'vali-loss', 0.17373966, 'vali-acc', 0.95703715)\n",
      "('Iteration: ', 2100, 'training-acc', 1.0, 'vali-loss', 0.1663563, 'vali-acc', 0.95788366)\n",
      "('Iteration: ', 2400, 'training-acc', 1.0, 'vali-loss', 0.16175134, 'vali-acc', 0.95978838)\n",
      "('Iteration: ', 2700, 'training-acc', 1.0, 'vali-loss', 0.16979502, 'vali-acc', 0.95978838)\n",
      "('Iteration: ', 3000, 'training-acc', 1.0, 'vali-loss', 0.17543599, 'vali-acc', 0.96074075)\n",
      "('Iteration: ', 3300, 'training-acc', 1.0, 'vali-loss', 0.19998176, 'vali-acc', 0.96455032)\n",
      "('Iteration: ', 3600, 'training-acc', 1.0, 'vali-loss', 0.19174238, 'vali-acc', 0.96455032)\n",
      "('Iteration: ', 3900, 'training-acc', 1.0, 'vali-loss', 0.21058181, 'vali-acc', 0.96455032)\n",
      "('Iteration: ', 4200, 'training-acc', 1.0, 'vali-loss', 0.21625096, 'vali-acc', 0.95978838)\n",
      "('Iteration: ', 4500, 'training-acc', 1.0, 'vali-loss', 0.23071395, 'vali-acc', 0.96550274)\n",
      "('Iteration: ', 4800, 'training-acc', 1.0, 'vali-loss', 0.21721768, 'vali-acc', 0.96370381)\n",
      "('Iteration: ', 5100, 'training-acc', 1.0, 'vali-loss', 0.22033755, 'vali-acc', 0.96846563)\n",
      "('Iteration: ', 5400, 'training-acc', 1.0, 'vali-loss', 0.29612982, 'vali-acc', 0.95978838)\n",
      "('Iteration: ', 5700, 'training-acc', 1.0, 'vali-loss', 0.22734863, 'vali-acc', 0.96740746)\n",
      "('Iteration: ', 6000, 'training-acc', 1.0, 'vali-loss', 0.24823655, 'vali-acc', 0.96740746)\n",
      "('Iteration: ', 6300, 'training-acc', 1.0, 'vali-loss', 0.25101405, 'vali-acc', 0.9664551)\n",
      "('Iteration: ', 6600, 'training-acc', 1.0, 'vali-loss', 0.30353254, 'vali-acc', 0.96455032)\n",
      "('Iteration: ', 6900, 'training-acc', 1.0, 'vali-loss', 0.26228571, 'vali-acc', 0.96264559)\n"
     ]
    }
   ],
   "source": [
    "batch_size=30\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Load the data\n",
    "    net.load('vggnet.npy', sess)\n",
    "    for i in range(7000):\n",
    "        arr = np.arange(len(data['X_train']))\n",
    "        np.random.shuffle(arr)\n",
    "        np_images, np_labels = (data['X_train'][arr[:batch_size]] ,data['y_train'][arr[:batch_size]])\n",
    "        feed = {images_raw: np_images, labels_raw: np_labels, keep_prob:0.4}\n",
    "        np_loss,acc, _ = sess.run([loss,accuracy, train_op], feed_dict=feed)\n",
    "        if i % 300 == 0:\n",
    "            idx = 0\n",
    "            loss_list = []\n",
    "            acc_list = []\n",
    "            while idx*batch_size < n_val :\n",
    "                range_idx=range(idx*batch_size, min( (idx+1)*batch_size,n_val))\n",
    "                np_images, np_labels = (data['X_val'][range_idx] ,data['y_val'][range_idx])\n",
    "                feed = {images_raw: np_images, labels_raw: np_labels, keep_prob:1}\n",
    "                vali_loss, vali_acc = sess.run([loss,accuracy], feed_dict=feed)\n",
    "                loss_list.append(vali_loss)\n",
    "                acc_list.append(vali_acc)\n",
    "                idx+=1\n",
    "            vali_loss_mean =  np.mean(loss_list)\n",
    "            vali_acc_mean =  np.mean(acc_list)\n",
    "            print('Iteration: ', i,\"training-acc\",acc,\"vali-loss\", vali_loss_mean, \"vali-acc\", vali_acc_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### only tune after pool5 without forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Hidden_n=1024\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "images_raw = tf.placeholder(tf.float32, shape=[None, 32,32,3])\n",
    "images = tf.image.resize_images(images_raw,(224,224))\n",
    "labels_raw = tf.placeholder(tf.int32, shape=[None,],name='truth_y')\n",
    "labels = tf.one_hot(labels_raw,n_class)\n",
    "\n",
    "# concate network\n",
    "net = MyNet({'data': images})\n",
    "pool5 = net.layers['pool5']\n",
    "with tf.name_scope('fintune_whimh'):\n",
    "    with tf.name_scope('fc6'):\n",
    "        pool5_flat_size = int(np.prod(pool5.shape[1:]))\n",
    "        pool5_flat = tf.reshape(pool5, [-1, pool5_flat_size])\n",
    "        W_fc6 = weight_variable([pool5_flat_size, Hidden_n])\n",
    "        b_fc6 = bias_variable([Hidden_n])\n",
    "        H_fc6=tf.nn.relu(tf.matmul(pool5_flat, W_fc6) + b_fc6) \n",
    "        H_fc6 = tf.nn.dropout(H_fc6, keep_prob)\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        #calculate_entropy\n",
    "        W_fc7 = weight_variable([Hidden_n, n_class])\n",
    "        b_fc7 = bias_variable([n_class])\n",
    "        fc7 = tf.matmul(H_fc6, W_fc7) + b_fc7 \n",
    "        \n",
    "        predctions = tf.argmax(fc7,1, name='predictions')\n",
    "        ground_truth = tf.argmax(labels,1)\n",
    "    with tf.name_scope('regular'):    \n",
    "        regularizers = (tf.nn.l2_loss(W_fc7) + tf.nn.l2_loss(b_fc7)+\n",
    "                        (tf.nn.l2_loss(W_fc6) + tf.nn.l2_loss(b_fc6)))\n",
    "correct_prediction = tf.equal(predctions, ground_truth)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=fc7 ,labels= labels), 0)+regularizers*1e-3\n",
    "#train_op = tf.train.AdamOptimizer(1e-6).minimize(loss)\n",
    "var_list2=[W_fc6,W_fc7,b_fc6,b_fc7]\n",
    "train_op = tf.train.AdamOptimizer(1e-6).minimize(loss,var_list=var_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Iteration: ', 0, 'training-acc', 0.40000004, 'vali-loss', 1.6083723, 'vali-acc', 0.75481492)\n",
      "('Iteration: ', 300, 'training-acc', 0.70000005, 'vali-loss', 0.78898078, 'vali-acc', 0.75481492)\n",
      "('Iteration: ', 600, 'training-acc', 0.9333334, 'vali-loss', 0.59451509, 'vali-acc', 0.79978848)\n",
      "('Iteration: ', 900, 'training-acc', 0.83333337, 'vali-loss', 0.49549621, 'vali-acc', 0.85788375)\n",
      "('Iteration: ', 1200, 'training-acc', 0.76666677, 'vali-loss', 0.42081538, 'vali-acc', 0.87216938)\n",
      "('Iteration: ', 1500, 'training-acc', 0.9666667, 'vali-loss', 0.36211753, 'vali-acc', 0.89894187)\n",
      "('Iteration: ', 1800, 'training-acc', 0.9000001, 'vali-loss', 0.31640065, 'vali-acc', 0.91703707)\n",
      "('Iteration: ', 2100, 'training-acc', 0.9333334, 'vali-loss', 0.28412145, 'vali-acc', 0.92465615)\n",
      "('Iteration: ', 2400, 'training-acc', 0.9666667, 'vali-loss', 0.25974858, 'vali-acc', 0.93608463)\n",
      "('Iteration: ', 2700, 'training-acc', 0.9666667, 'vali-loss', 0.23863435, 'vali-acc', 0.94084656)\n",
      "('Iteration: ', 3000, 'training-acc', 0.9333334, 'vali-loss', 0.22399862, 'vali-acc', 0.94275141)\n",
      "('Iteration: ', 3300, 'training-acc', 0.9000001, 'vali-loss', 0.21129782, 'vali-acc', 0.94571435)\n",
      "('Iteration: ', 3600, 'training-acc', 0.9666667, 'vali-loss', 0.20304747, 'vali-acc', 0.94666684)\n",
      "('Iteration: ', 3900, 'training-acc', 0.9666667, 'vali-loss', 0.19544864, 'vali-acc', 0.94952393)\n",
      "('Iteration: ', 4200, 'training-acc', 0.9666667, 'vali-loss', 0.18832685, 'vali-acc', 0.95238101)\n",
      "('Iteration: ', 4500, 'training-acc', 1.0, 'vali-loss', 0.18272977, 'vali-acc', 0.9552381)\n",
      "('Iteration: ', 4800, 'training-acc', 0.9666667, 'vali-loss', 0.18068893, 'vali-acc', 0.95428574)\n",
      "('Iteration: ', 5100, 'training-acc', 1.0, 'vali-loss', 0.17462836, 'vali-acc', 0.95714283)\n",
      "('Iteration: ', 5400, 'training-acc', 1.0, 'vali-loss', 0.17213857, 'vali-acc', 0.95714283)\n",
      "('Iteration: ', 5700, 'training-acc', 0.9666667, 'vali-loss', 0.16661987, 'vali-acc', 0.96095234)\n",
      "('Iteration: ', 6000, 'training-acc', 1.0, 'vali-loss', 0.16557761, 'vali-acc', 0.95904762)\n",
      "('Iteration: ', 6300, 'training-acc', 1.0, 'vali-loss', 0.16452242, 'vali-acc', 0.96095234)\n",
      "('Iteration: ', 6600, 'training-acc', 0.9666667, 'vali-loss', 0.15953395, 'vali-acc', 0.96190482)\n",
      "('Iteration: ', 6900, 'training-acc', 1.0, 'vali-loss', 0.15827124, 'vali-acc', 0.9619047)\n",
      "('Iteration: ', 7200, 'training-acc', 1.0, 'vali-loss', 0.15789808, 'vali-acc', 0.9619047)\n",
      "('Iteration: ', 7500, 'training-acc', 1.0, 'vali-loss', 0.15735023, 'vali-acc', 0.96285719)\n",
      "('Iteration: ', 7800, 'training-acc', 1.0, 'vali-loss', 0.15487978, 'vali-acc', 0.9619047)\n",
      "('Iteration: ', 8100, 'training-acc', 1.0, 'vali-loss', 0.15817714, 'vali-acc', 0.96095234)\n",
      "('Iteration: ', 8400, 'training-acc', 1.0, 'vali-loss', 0.15441923, 'vali-acc', 0.95999998)\n",
      "('Iteration: ', 8700, 'training-acc', 1.0, 'vali-loss', 0.15448885, 'vali-acc', 0.96095234)\n",
      "('Iteration: ', 9000, 'training-acc', 1.0, 'vali-loss', 0.15540278, 'vali-acc', 0.96095234)\n",
      "('Iteration: ', 9300, 'training-acc', 1.0, 'vali-loss', 0.15420043, 'vali-acc', 0.95999998)\n",
      "('Iteration: ', 9600, 'training-acc', 1.0, 'vali-loss', 0.15354498, 'vali-acc', 0.96095234)\n",
      "('Iteration: ', 9900, 'training-acc', 1.0, 'vali-loss', 0.15510018, 'vali-acc', 0.95999998)\n",
      "('Iteration: ', 10200, 'training-acc', 1.0, 'vali-loss', 0.15824389, 'vali-acc', 0.95904762)\n",
      "('Iteration: ', 10500, 'training-acc', 1.0, 'vali-loss', 0.15774612, 'vali-acc', 0.95999998)\n",
      "('Iteration: ', 10800, 'training-acc', 1.0, 'vali-loss', 0.1624466, 'vali-acc', 0.95999998)\n",
      "('Iteration: ', 11100, 'training-acc', 1.0, 'vali-loss', 0.15930025, 'vali-acc', 0.96095234)\n",
      "('Iteration: ', 11400, 'training-acc', 1.0, 'vali-loss', 0.15837026, 'vali-acc', 0.9619047)\n",
      "('Iteration: ', 11700, 'training-acc', 1.0, 'vali-loss', 0.1641283, 'vali-acc', 0.96190482)\n"
     ]
    }
   ],
   "source": [
    "batch_size=30\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "     # Load the layers\n",
    "    net.load('vggnet.npy', sess)\n",
    "    for i in range(12000):\n",
    "        arr = np.arange(len(data['X_train']))\n",
    "        np.random.shuffle(arr)\n",
    "        np_images, np_labels = (data['X_train'][arr[:batch_size]] ,data['y_train'][arr[:batch_size]])\n",
    "        feed = {images_raw: np_images, labels_raw: np_labels, keep_prob:0.4}\n",
    "        np_loss,acc, _ = sess.run([loss,accuracy, train_op], feed_dict=feed)\n",
    "        if i % 300 == 0:\n",
    "            idx = 0\n",
    "            loss_list = []\n",
    "            acc_list = []\n",
    "            while idx*batch_size < n_val :\n",
    "                range_idx=range(idx*batch_size, min( (idx+1)*batch_size,n_val))\n",
    "                np_images, np_labels = (data['X_val'][range_idx] ,data['y_val'][range_idx])\n",
    "                feed = {images_raw: np_images, labels_raw: np_labels, keep_prob:1}\n",
    "                vali_loss, vali_acc = sess.run([loss,accuracy], feed_dict=feed)\n",
    "                loss_list.append(vali_loss)\n",
    "                acc_list.append(vali_acc)\n",
    "                idx+=1\n",
    "            vali_loss_mean =  np.mean(loss_list)\n",
    "            vali_acc_mean =  np.mean(acc_list)\n",
    "            print('Iteration: ', i,\"training-acc\",acc,\"vali-loss\", vali_loss_mean, \"vali-acc\", vali_acc_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can notice that fine tuning with hole network can reduce the loss more faster, but overfit faster\n",
    "\n",
    "## but with  pooling5 freezed can  reach a lower loss than whole network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
